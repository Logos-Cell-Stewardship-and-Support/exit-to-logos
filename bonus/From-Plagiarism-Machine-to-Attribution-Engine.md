---
title: "From Plagiarism Machine to Attribution Engine"
date: 2025-06-21
description: Reflections on AI voices, surveillance creep, and how to build attribution engines and public infrastructure instead of copy machines for the powerful.
tags: [AI, attribution, local models, ElevenLabs, Codex, Nomos, Waku, ethical AI, public infrastructure, mesh networks, LOGOS]
---

Lately, for the first time in years, I’ve found myself buying *e‑books* again.

Not because I suddenly enjoy reading off a screen, but because tools like ElevenLabs are letting me flip between different voices reading the same text. Burt Reynolds doing political theory. Maya Angelou reading governance experiments. I follow along visually and it’s weirdly satisfying. I’m not sure if I really *have* agency, but it feels like I do, and that’s something.

But every time I hear those voices, I wonder: **who’s getting paid?**

If someone sold the rights to Burt Reynolds’ voice, his name, face, whole cowboy vibe, are they getting micropayments when people use it? When Maya Angelou reads a chapter of *Governable Spaces*, does her estate get a cut? Is anyone even tracking this?

This is where **attribution modeling** stops being theoretical and starts being structural. This should be built into every layer. If AI is going to remix and impersonate and regenerate content, then the systems that *run* that content should also **track it, audit it, and pay people fairly.**

If we got this right, and I know that’s a big “if,” you’d have the foundations of something like universal basic income. Not as charity. As **compensation**. Every pause, reaction, remix, and data point is a contribution to a system that’s constantly learning. That’s value. So... let’s treat it like value.

---

## Flip the default: plagiarism machine → attribution engine

**What if AI wasn’t built defensively to avoid lawsuits, but proactively to make sure people get paid?  
What if the default wasn’t plagiarism, but attribution, and even compensation?**

That’s the framing I keep coming back to. Not as a pie in the sky utopia, but as a legit design challenge. What would it take to build AI systems that treat people as contributors to shared knowledge, not just training data to be scraped?

---

## ML isn’t going away. The question is what we build with it.

Yes, there are serious and valid concerns about surveillance and militarization. Companies like **Palantir**, **OpenAI**, **Google**, **Anduril**, and **Clearview AI** are already working hand in hand with state governments to deploy AI for predictive policing, biometric surveillance, military targeting, and other forms of systemic violence. This isn't speculative. It’s real. It’s happening now.

We’re talking about real-time analytics used to police neighborhoods, scrape faces, track dissidents, automate drone strikes, and gatekeep borders and public services. All under the branding of “efficiency” and “security.” It’s dystopian, not because it’s futuristic, but because it’s already here.

And let’s be honest. **Machine learning is everywhere now**. It’s in every photo we take. It’s running in the background of nearly every app and website we use. It’s scraping everything, **including museum archives**, to the point that some museum websites are literally crashing under the weight of the bots. Cultural memory, turned training fodder.

So the question isn’t whether AI is going to shape the future. It’s whether we’re **setting up parallel systems**, rooted in public good and community control, to push back against the extractive defaults. If you care about rights, autonomy, or just having a say in how the tools around you work, the only real move is to start **building alternatives** that don’t require surrendering your data, your voice, or your agency.

---

## This isn’t sci-fi. It’s ethical infrastructure.

Ethical AI researchers, like Maren Smith and many others, have been asking this for years. What would it look like if AI was built as **public infrastructure**? Not just made “less harmful,” but fundamentally oriented toward justice and sustainability?

Groups like the Alan Turing Institute’s FATE initiative and scholars in the *Cambridge Handbook of Responsible AI* have pushed for **ethical implementation**, not just high-level statements. This is what that looks like in practice.

---

## What it could look like

| Feature                    | Description                                                  |
|----------------------------|--------------------------------------------------------------|
| Local, private models      | Containerized AI run on your own machines                   |
| Transparent usage logs     | Codex tracks events for verifiable attribution              |
| Smart-contract remittance  | Nomos handles fair payments to contributors                 |
| Decentralized sync         | Waku lets peers talk without cloud dependencies             |
| Micro-economies everywhere | Everyone gets credit for the signals they generate, consciously or not |

---

## Exploring the LOGOS stack

But exactly which tools are best suited for this work?

That’s part of why I’ve been exploring tools like **Codex**, **Nomos**, and **Waku** lately.

They come out of the LOGOS ecosystem, and I think they represent one of the more serious attempts to rethink some fundamentally important infrastructure as open source, privacy by default.

I’ve been thinking through what happens *when* the internet gets cut off, like it does in conflict zones or authoritarian crackdowns, and how we might still **coordinate, compensate, and publish**, even when centralized networks fail.

Whether we’re talking about universities in mountain regions, urban mesh projects, or communities surviving war, the real question is: *what should resilient, privacy-first infrastructure actually look like?*

---

## Run it locally. Share it globally.

Here’s the dream stack:

- **Local AI models**, run on your own hardware  
- Containerized tools you control (Ollama, Docker, Flatpaks, whatever)  
- AI agents shared peer to peer, no cloud necessary  
- Your voice, your weights, your data, your call  

**Radical transparency for institutions and the powerful.  
Privacy by default for the rest of us.**

---

## Coordinate with Codex, Nomos, and Waku

**Codex**, **Nomos**, and **Waku** aren’t glued together. They’re designed to be composable, interoperable building blocks for resilient systems. Each one does a specific job well, and they work better together without locking anyone in.

- **Codex** tracks attribution, datasets, and interactions. So we can *know* who contributed to what  
- **Nomos** encodes the rules. Who gets paid, how much, when, and why. It’s programmable social contracts  
- **Waku** handles the messaging, even over local networks or mesh. No Google Cloud, no AWS. Just peers

Using them in tandem, we can:

- Automate attribution and micropayments  
- Track model use in a decentralized way  
- Respect privacy *while* enabling fair compensation  
- Coordinate globally, even without internet access  
- Build a cultural memory that *pays back* the people it came from

---

## Final thought

This isn’t just a blog post. It’s a blueprint.

We can’t stop the use of AI. But we can build systems where creators, researchers, remixers, dataset curators, even passive users, get **compensated** for their contributions. Where the tools work for us, not against us.

The genie’s out of the bottle.  
But maybe, just maybe, we can teach it to leave a tip.
